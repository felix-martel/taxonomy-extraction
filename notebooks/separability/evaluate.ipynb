{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from warnings import filterwarnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import libs.embeddings as embeddings\n",
    "from libs.utils import Timer\n",
    "from libs.graph import KnowledgeGraph\n",
    "from libs.taxonomy import Taxonomy\n",
    "from libs.separability.evaluation import class_distance, evaluate\n",
    "from libs.separability.data import get_centroids\n",
    "\n",
    "\n",
    "kg = KnowledgeGraph.from_dir(\"data/dbpedia\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classes to separate\n",
    "\n",
    "fa = \"data/separability/types_A.txt\"\n",
    "fb = \"data/separability/types_B.txt\"\n",
    "missing_ids = \"data/missing_ids.txt\"\n",
    "\n",
    "with open(fa, \"r\") as f:\n",
    "    As = f.read().split()\n",
    "    \n",
    "with open(fb, \"r\") as f:\n",
    "    Bs = f.read().split()\n",
    "    \n",
    "with open(missing_ids, \"r\") as f:\n",
    "    invalid_ids = {int(x) for x in f.read().split()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances between classes\n",
    "\n",
    "T = Taxonomy.load(\"full\")\n",
    "centroids, counts = get_centroids(kg, T)\n",
    "class_distance = partial(class_distance, tax=T, centroids=centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filterwarnings(\"ignore\")  # Ignore ill-defined precision warnings\n",
    "\n",
    "size = 300\n",
    "min_size = 10\n",
    "results = {}\n",
    "verbose = False\n",
    "\n",
    "df = None\n",
    "MODELS = [\"ComplEx\", \"DistMult\", \"RDF2Vec\", \"TransE\", \"TransH\", \"TransD\"]\n",
    "\n",
    "for model in MODELS:\n",
    "    E = embeddings.load(model)\n",
    "    _, dim = E.shape\n",
    "    stop = False\n",
    "    for a in tqdm(As, desc=model):\n",
    "        if a == \"owl:Thing\": \n",
    "            continue\n",
    "        all_ias = kg.sample_instances(size, a, force_size=False, exclude_ids=invalid_ids)\n",
    "        all_na = len(all_ias)\n",
    "        \n",
    "        for b in Bs:\n",
    "            if b == a or b == \"owl:Thing\": \n",
    "                continue\n",
    "            if (b, a) in results: \n",
    "                results[(a,b)] = results[(b, a)]\n",
    "                continue\n",
    "            params = {}\n",
    "            if T[b] < T[a]:\n",
    "                params[\"except_type\"] = a\n",
    "            ias = kg.sample_instances(size, a, force_size=False, except_type=b, exclude_ids=invalid_ids) if T[a] < T[b] \\\n",
    "                else all_ias\n",
    "            ibs = kg.sample_instances(size, b, force_size=False, exclude_ids=invalid_ids, **params)\n",
    "            na, nb = len(ias), len(ibs)\n",
    "            if na < min_size or nb < min_size: \n",
    "                continue\n",
    "            indices = [*ias, *ibs]\n",
    "            y = np.concatenate([np.zeros(na), np.ones(nb)])\n",
    "            X = E[indices]\n",
    "            res = evaluate(X, y)\n",
    "            dist = class_distance(a, b)\n",
    "            \n",
    "            results[(a,b)] = {\"a\": a, \"b\": b, \"ca\": counts[a], \"cb\": counts[b], **dist, **res}\n",
    "                \n",
    "        curr_df = pd.DataFrame(results.values())\n",
    "        curr_df[\"model\"] = model\n",
    "        if df is None:\n",
    "            df = curr_df\n",
    "        else:\n",
    "            df = pd.concat([df, curr_df])\n",
    "        \n",
    "df.to_csv(f\"results/separability/all.csv\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we compute the lexical distance between classes. The embedding file needs to be downloaded from [https://fasttext.cc/docs/en/english-vectors.html](https://fasttext.cc/docs/en/english-vectors.html) (use the Common Crawl, 600B tokens version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import itertools as it\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.spatial.distance import euclidean, cosine\n",
    "\n",
    "\n",
    "# Step 1: load embedding vectors\n",
    "with open(\"data/word_embeddings/crawl-300d-2M.vec\", \"r\") as f:\n",
    "    n_words, dim = map(int, next(f).split())\n",
    "    E = np.zeros((n_words, dim))\n",
    "    words = {}\n",
    "    for i, line in enumerate(f):\n",
    "        word, *vec = line.split()\n",
    "        words[word] = i\n",
    "        E[i] = np.array(vec, dtype=float)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_case_split(s):  \n",
    "    return [s.lower() for s in re.findall(r'[A-Z](?:[a-z]+|[A-Z]*(?=[A-Z]|$))', s)]\n",
    "\n",
    "def extract_keywords(cls):\n",
    "    return camel_case_split(cls.replace(\"dbo:\", \"\").replace(\"owl:\", \"\"))\n",
    "\n",
    "assert extract_keywords(\"dbo:ClericalAdministrativeRegion\") == (\"clerical\", \"administrative\", \"region\")\n",
    "\n",
    "# Step 2: retrieve all classes in the dataframe and split them into keywords\n",
    "classes = set(df.a.unique()) | set(df.b.unique())\n",
    "vocab = set(it.chain(*[extract_keywords(cls) for cls in classes]))\n",
    "assert not vocab - word.keys()\n",
    "\n",
    "# Step 3: average word embeddings to get the vector representation of each class\n",
    "class_vector = {}\n",
    "for c in classes:\n",
    "    kw = extract_keywords(c)\n",
    "    class_vector[c] = sum(E[words[k]] for k in kw) / len(kw)\n",
    "    \n",
    "class_vector[\"owl:Thing\"] = E[words[\"thing\"]]\n",
    "\n",
    "# Step 4: define distances over class names\n",
    "def word_distance(l):\n",
    "    a, b = l\n",
    "    return euclidean(class_vector[a], class_vector[b])\n",
    "\n",
    "def word_cdistance(l):\n",
    "    a, b = l\n",
    "    return cosine(class_vector[a], class_vector[b])\n",
    "\n",
    "def word_norm_distance(l):\n",
    "    a, b = l\n",
    "    va, vb = class_vector[a], class_vector[b]\n",
    "    return euclidean(va/np.linalg.norm(va), vb/np.linalg.norm(vb))\n",
    "\n",
    "word_norm_distance([\"dbo:OfficeHolder\", \"dbo:Politician\"])  # This should be around 1.167\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"hsize\"] = df[[\"ca\", \"cb\"]].apply(stats.hmean, axis=1)\n",
    "df[\"gsize\"] = df[[\"ca\", \"cb\"]].apply(stats.gmean, axis=1)\n",
    "df[\"cos\"] = df[[\"a\", \"b\"]].apply(word_cdistance, axis=1)\n",
    "df[\"euc\"] = df[[\"a\", \"b\"]].apply(word_distance, axis=1)\n",
    "df[\"neuc\"] = df[[\"a\", \"b\"]].apply(word_norm_distance, axis=1)\n",
    "df[\"taxcos\"] = 0.5 * (df.taxo/df.taxo.max() + df.cos/df.cos.max())\n",
    "df[\"taxeuc\"] = 0.5 * (df.taxo/df.taxo.max() + df.euc/df.euc.max())\n",
    "df[\"taxneuc\"] = 0.5 * (df.taxo/df.taxo.max() + df.neuc/df.neuc.max())\n",
    "\n",
    "fname = \"results/separability/all_sized.csv\"\n",
    "df.to_csv(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
