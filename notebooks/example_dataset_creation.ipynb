{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation\n",
    "\n",
    "This notebook creates a `Dataset` from a knowledge graph.\n",
    "\n",
    "A small subgraph of DBpedia is stored in the OpenKE format in `data/dbpedia/toy`. This subgraph, denoted `DBP50K`, contains:\n",
    "- 54,795 entities\n",
    "- 776 relations\n",
    "- 316,114 triples\n",
    "\n",
    "In this notebook, we load this graph, create a `Dataset` by sampling instances from the graph, and store it on disk. This illustrates the use of classes `KnowledgeGraph`, `Taxonomy` and `Dataset`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 1** Load the graph from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Triples: 100%|██████████| 316114/316114 [00:02<00:00, 118048.13it/s]\n"
     ]
    }
   ],
   "source": [
    "from libs.graph import KnowledgeGraph\n",
    "\n",
    "kg = KnowledgeGraph.from_dir(\"toy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 2** In our setting, the graph contains no taxonomic information, i.e no `rdfs:subClassOf` relations.\n",
    "Our gold standard must thus come from an external source. Here, we use the axioms stored in `data/taxonomy/toy.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ┌dbo:Location┐\n",
      "     │            └dbo:PopulatedPlace┐\n",
      "     │                               └dbo:Settlement\n",
      " root┤\n",
      "     │         ┌dbo:Organisation\n",
      "     ├dbo:Agent┤\n",
      "     │         └dbo:Person┐\n",
      "     │                    └dbo:Athlete\n",
      "     │         ┌dbo:SocietalEvent\n",
      "     └dbo:Event┤\n",
      "               └dbo:SportsEvent\n"
     ]
    }
   ],
   "source": [
    "from libs.taxonomy import Taxonomy\n",
    "\n",
    "T = Taxonomy.from_file(\"data/taxonomy/toy.txt\", add_root=\"root\")\n",
    "T.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 3** For each class in the taxonomy, sample entites and add them to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dbo:Agent',\n",
       " 'dbo:Athlete',\n",
       " 'dbo:Event',\n",
       " 'dbo:Location',\n",
       " 'dbo:Organisation',\n",
       " 'dbo:Person',\n",
       " 'dbo:PopulatedPlace',\n",
       " 'dbo:Settlement',\n",
       " 'dbo:SocietalEvent',\n",
       " 'dbo:SportsEvent'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes = {cls.name for cls in T if not cls.is_root}\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.dataset import Dataset\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "n_classes = 10\n",
    "n_entities = 200\n",
    "\n",
    "used_indices = set()\n",
    "indices = []\n",
    "labels = []\n",
    "name2cls, cls2name = dict(), dict()\n",
    "\n",
    "for name in classes:\n",
    "    cls = len(name2cls)\n",
    "    name2cls[name] = cls\n",
    "    cls2name[cls] = name\n",
    "    \n",
    "    for instance in kg.sample_instances(n_entities, from_type=name, exclude_ids=used_indices):\n",
    "        used_indices.add(instance)\n",
    "        indices.append(instance)\n",
    "        labels.append(cls)\n",
    "        \n",
    "indices, labels = shuffle(indices, labels)\n",
    "\n",
    "data = Dataset(indices, labels, name2cls, cls2name, axioms=T.to_axioms())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is now created. A summary can be printed using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset (10 classes, 2000 instances):\n",
      "---\n",
      "dbo:Settlement       200\n",
      "dbo:Location         200\n",
      "dbo:SportsEvent      200\n",
      "dbo:PopulatedPlace   200\n",
      "dbo:Athlete          200\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "print(data.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step 4** Save dataset (it will then become accessible with `Dataset.load('data/dataset/toy')`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "dirname = \"data/dataset/toy\"\n",
    "if not os.path.exists(dirname):\n",
    "    data.save(dirname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
